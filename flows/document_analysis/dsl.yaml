flow_name: "document_analysis"
description: "Extract text from documents and analyze with LLM"

steps:
  - name: "extract_text"
    type: "document_extraction"
    description: "Extract text from document file"
    config:
      provider: "${inputs.document_provider}"
      chunk_text: "${inputs.chunk_text}"
      chunk_size: 1000
      chunk_overlap: 200
    inputs:
      file_path: "${inputs.file_path}"
    outputs:
      - "extracted_text"
      - "metadata"
      - "chunks"

  - name: "prepare_analysis_prompt"
    type: "template"
    description: "Prepare analysis prompt based on analysis type"
    depends_on: ["extract_text"]
    config:
      template_logic: |
        if analysis_type == "summary":
            return "Please provide a concise summary of the following document:\n\n{text}"
        elif analysis_type == "comprehensive":
            return "Please provide a comprehensive analysis of the following document including:\n1. Summary\n2. Key points\n3. Main themes\n4. Important entities\n5. Conclusions\n\nDocument:\n{text}"
        elif analysis_type == "key_points":
            return "Please extract and list the key points from the following document:\n\n{text}"
        elif analysis_type == "entities":
            return "Please identify and extract important entities (people, organizations, dates, locations, etc.) from the following document:\n\n{text}"
        elif analysis_type == "custom":
            return custom_prompt + "\n\nDocument:\n{text}"
        else:
            return "Please analyze the following document:\n\n{text}"
    inputs:
      analysis_type: "${inputs.analysis_type}"
      custom_prompt: "${inputs.custom_prompt}"
    outputs:
      - "analysis_prompt"

  - name: "analyze_document"
    type: "llm"
    description: "Analyze extracted text with LLM"
    depends_on: ["extract_text", "prepare_analysis_prompt"]
    config:
      provider: "ollama"
      model: "${inputs.llm_model}"
      temperature: 0.3
      max_tokens: 2000
    inputs:
      prompt: "${steps.prepare_analysis_prompt.outputs.analysis_prompt}"
      text: "${steps.extract_text.outputs.extracted_text}"
      chunks: "${steps.extract_text.outputs.chunks}"
    outputs:
      - "analysis_result"

  - name: "format_results"
    type: "template"
    description: "Format final results"
    depends_on: ["extract_text", "analyze_document"]
    config:
      template: |
        {
          "document_info": {
            "file_path": "${inputs.file_path}",
            "extraction_provider": "${inputs.document_provider}",
            "analysis_type": "${inputs.analysis_type}",
            "llm_model": "${inputs.llm_model}"
          },
          "extracted_text": "${steps.extract_text.outputs.extracted_text}",
          "metadata": ${steps.extract_text.outputs.metadata},
          "analysis": "${steps.analyze_document.outputs.analysis_result}",
          "chunks": ${steps.extract_text.outputs.chunks}
        }
    outputs:
      - "final_result"

# Flow outputs
outputs:
  extracted_text: "${steps.extract_text.outputs.extracted_text}"
  analysis: "${steps.analyze_document.outputs.analysis_result}"
  metadata: "${steps.extract_text.outputs.metadata}"
  chunks: "${steps.extract_text.outputs.chunks}"
  final_result: "${steps.format_results.outputs.final_result}"
