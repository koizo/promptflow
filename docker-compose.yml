version: '3.8'
services:
  app:
    build: .
    ports:
    - 8000:8000
    depends_on:
    - redis
    environment:
    - REDIS_URL=redis://redis:6379
    - OLLAMA_BASE_URL=http://host.docker.internal:11434
    volumes:
    - ./logs:/app/logs
    - temp_files:/app/temp
  redis:
    image: redis:7-alpine
    ports:
    - 6379:6379
    volumes:
    - redis_data:/data
    command: redis-server --appendonly yes
  celery-worker-ocr_analysis:
    build: .
    command: celery -A celery_app worker --loglevel=info -Q ocr_analysis --concurrency=2
    depends_on:
    - redis
    environment:
    - REDIS_URL=redis://redis:6379
    - FLOW_NAME=ocr_analysis
    - MAX_CONCURRENT=2
    - QUEUE_NAME=ocr_analysis
    - OLLAMA_BASE_URL=http://host.docker.internal:11434
    volumes:
    - ./logs:/app/logs
    - temp_files:/app/temp
  celery-flower:
    build: .
    command: celery -A celery_app flower --port=5555
    ports:
    - 5555:5555
    depends_on:
    - redis
    environment:
    - REDIS_URL=redis://redis:6379
    - OLLAMA_BASE_URL=http://host.docker.internal:11434
volumes:
  redis_data: {}
  temp_files: {}
networks:
  default:
    name: ai-inference-platform
