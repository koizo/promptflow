# AI Inference Platform

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)

Build AI workflows with YAML configuration. Combine OCR, document processing, and LLM analysis through reusable executors that auto-generate REST APIs.

## Case Study: Amazon Q Developer Evaluation

This project serves as a case study to evaluate **Amazon Q Developer** as an autonomous development agent using various MCP servers. While all code was fully generated by Amazon Q, strict rules and guidance were essential to reach this level of maturity.

## Features

- **YAML-Driven Workflows** - Define AI flows using simple YAML configuration
- **Auto-Generated APIs** - REST endpoints created automatically from flow definitions
- **Async Processing** - Celery-based task processing for production workloads
- **Multi-Format Support** - Handle documents, images, and text files
- **Real-Time Status** - Track flow execution progress via Redis state management

## Architecture

```
YAML Flow → Flow Engine → Executors → Auto-Generated API
API -> Celery -> Executors
```

The platform uses **Executors** (reusable AI components) orchestrated by **Flows** (YAML definitions) to automatically generate REST APIs.

## Quick Start

1. **Start the platform**
```bash
git clone <repository>
cd promptflow
docker-compose up -d
```

2. **View documentation**
- API Docs: http://localhost:8000/docs
- Flow Catalog: http://localhost:8000/catalog

## Flow Definition

Define workflows in YAML:

```yaml
name: "document_analysis"
description: "Extract and analyze text from documents"

inputs:
  - name: "file"
    type: "file"
    required: true

steps:
  - name: "extract_text"
    executor: "document_extractor"
    config:
      file_path: "{{ inputs.file }}"
      
  - name: "analyze_content"
    executor: "llm_analyzer"
    config:
      text: "{{ steps.extract_text.text }}"

config:
  execution:
    mode: "async"
    timeout: 300
```

## Execution Modes

### Asynchronous (Production)
```yaml
config:
  execution:
    mode: "async"
```

- **Non-blocking** - Returns task ID immediately
- **Celery workers** - Processing in dedicated worker processes
- **Scalable** - Handle multiple concurrent requests
- **Status tracking** - Real-time progress via Redis
- **Recommended for production**

### Synchronous (Development Only)
```yaml
config:
  execution:
    mode: "sync"
```

**⚠️ NOT RECOMMENDED FOR PRODUCTION**

- **Blocking** - Blocks API server during processing
- **No concurrency** - Cannot handle multiple requests
- **Timeout risk** - Long tasks may fail
- **Development only** - Use for testing and debugging


## Core Executors

### File Processing
- **FileHandler** - Upload, validation, temp storage
- **DocumentExtractor** - Text extraction from documents
- **ImageHandler** - Image processing

### AI Processing
- **OCRProcessor** - Optical Character Recognition
- **WhisperProcessor** - Speech-to-text transcription (Local, OpenAI, HuggingFace)
- **LLMAnalyzer** - Large Language Model analysis

### Utility
- **ResponseFormatter** - Standardized responses
- **TemplateEngine** - Dynamic configuration

## Extending the Platform

### 1. Create Executor
```python
from .base_executor import BaseExecutor, ExecutionResult

class MyExecutor(BaseExecutor):
    async def execute(self, context):
        result = await self.process_data(context.config.get("input"))
        return ExecutionResult(success=True, data={"output": result})
```

### 2. Register Executor
```python
self.executor_registry.register("my_executor", MyExecutor)
```

### 3. Create Flow
```yaml
name: "my_flow"
steps:
  - name: "process"
    executor: "my_executor"
    config:
      input: "{{ inputs.data }}"
```

### 4. Auto-Generated APIs
- `POST /api/v1/my-flow/execute`
- `GET /api/v1/my-flow/info`
- `GET /api/v1/my-flow/health`

## Container Architecture

```yaml
services:
  app:              # FastAPI application
  redis:            # State storage
  celery-worker-*:  # Async processing workers
  celery-flower:    # Worker monitoring
```

## Testing

```bash
# Health check
curl http://localhost:8000/health

# Flow catalog
curl http://localhost:8000/catalog

# Unit tests
pytest tests/ -v
```

## Project Structure

```
promptflow/
├── core/
│   ├── executors/          # AI components
│   ├── flow_engine/        # Flow orchestration
│   ├── schema.py          # Data models
│   └── state_store.py     # Redis state management
├── flows/                 # YAML flow definitions
├── tests/                 # Test suite
├── main.py               # FastAPI application
├── celery_app.py         # Celery configuration
└── docker-compose.yml    # Container setup
```

## Prerequisites

- Docker & Docker Compose
- Python 3.11+ (for local development)
- Ollama (for LLM inference)

## License

MIT License - see [LICENSE](LICENSE) file for details.