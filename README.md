# AI Inference Platform

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)

Build AI workflows with YAML configuration. Combine OCR, document processing, and LLM analysis through reusable executors that auto-generate REST APIs.

## Case Study: Amazon Q Developer Evaluation

This project serves as a case study to evaluate **Amazon Q Developer** as an autonomous development agent using various MCP servers. While all code was fully generated by Amazon Q, strict rules and guidance were essential to reach this level of maturity.

## Features

- **YAML-Driven Workflows** - Define AI flows using simple YAML configuration
- **Auto-Generated APIs** - REST endpoints created automatically from flow definitions
- **Async Processing** - Celery-based task processing for production workloads
- **Multi-Format Support** - Handle documents, images, and text files
- **Real-Time Status** - Track flow execution progress via Redis state management

## Architecture

```
YAML Flow ‚Üí Flow Engine ‚Üí Executors ‚Üí Auto-Generated API
API -> Celery -> Executors
```

The platform uses **Executors** (reusable AI components) orchestrated by **Flows** (YAML definitions) to automatically generate REST APIs.

## Quick Start

1. **Start the platform**
```bash
git clone <repository>
cd promptflow
docker-compose up -d
```

2. **View documentation**
- API Docs: http://localhost:8000/docs
- Flow Catalog: http://localhost:8000/catalog

## Flow Definition

Define workflows in YAML:

```yaml
name: "document_analysis"
description: "Extract and analyze text from documents"

inputs:
  - name: "file"
    type: "file"
    required: true

steps:
  - name: "extract_text"
    executor: "document_extractor"
    config:
      file_path: "{{ inputs.file }}"
      
  - name: "analyze_content"
    executor: "llm_analyzer"
    config:
      text: "{{ steps.extract_text.text }}"

config:
  execution:
    mode: "async"
    timeout: 300
```

## Execution Modes

### Asynchronous (Production)
```yaml
config:
  execution:
    mode: "async"
```

- **Non-blocking** - Returns task ID immediately
- **Celery workers** - Processing in dedicated worker processes
- **Scalable** - Handle multiple concurrent requests
- **Status tracking** - Real-time progress via Redis
- **Recommended for production**

### Synchronous (Development Only)
```yaml
config:
  execution:
    mode: "sync"
```

**‚ö†Ô∏è NOT RECOMMENDED FOR PRODUCTION**

- **Blocking** - Blocks API server during processing
- **No concurrency** - Cannot handle multiple requests
- **Timeout risk** - Long tasks may fail
- **Development only** - Use for testing and debugging


## Core Executors

### File Processing
- **FileHandler** - Upload, validation, temp storage
- **DocumentExtractor** - Text extraction from documents
- **ImageHandler** - Image processing

### AI Processing
- **OCRProcessor** - Optical Character Recognition
- **WhisperProcessor** - Speech-to-text transcription (Local, OpenAI, HuggingFace)
- **SentimentAnalyzer** - Sentiment and emotion analysis (HuggingFace, LLM)
- **LLMAnalyzer** - Large Language Model analysis

### Utility
- **ResponseFormatter** - Standardized responses
- **TemplateEngine** - Dynamic configuration

## Extending the Platform

### 1. Create Executor
```python
from .base_executor import BaseExecutor, ExecutionResult

class MyExecutor(BaseExecutor):
    async def execute(self, context):
        result = await self.process_data(context.config.get("input"))
        return ExecutionResult(success=True, data={"output": result})
```

### 2. Register Executor
```python
self.executor_registry.register("my_executor", MyExecutor)
```

### 3. Create Flow
```yaml
name: "my_flow"
steps:
  - name: "process"
    executor: "my_executor"
    config:
      input: "{{ inputs.data }}"
```

### 4. Auto-Generated APIs
- `POST /api/v1/my-flow/execute`
- `GET /api/v1/my-flow/info`
- `GET /api/v1/my-flow/health`

## Sentiment Analysis

The platform includes comprehensive sentiment analysis capabilities with dual provider architecture for different use cases.

### Provider Options

#### HuggingFace Provider (Recommended for Speed)
- **Best for**: High-volume processing, real-time analysis
- **Models**: Specialized sentiment models (cardiffnlp/twitter-roberta-base-sentiment-latest)
- **Performance**: 0.02-0.05 seconds per analysis
- **Accuracy**: 95-99% confidence on clear sentiment cases
- **Device Support**: Auto-detection (CPU/CUDA)

#### LLM Provider (Recommended for Nuanced Analysis)
- **Best for**: Complex text, detailed analysis, custom prompts
- **Models**: Reuses existing LLM infrastructure (Ollama, OpenAI)
- **Performance**: 2-5 seconds per analysis
- **Accuracy**: Context-aware, handles nuanced sentiment
- **Features**: Aspect-based analysis, emotion detection

### Analysis Types

```yaml
# Basic sentiment analysis
analysis_type: "basic"          # positive/negative/neutral + confidence

# Detailed analysis with emotions
analysis_type: "detailed"       # + emotions, key phrases, reasoning

# Comprehensive analysis
analysis_type: "comprehensive"  # + emotion scores, aspects, insights

# Emotion-focused analysis
analysis_type: "emotions"       # Detailed emotion detection
```

### Usage Examples

#### Quick Sentiment Check (HuggingFace)
```bash
curl -X POST "http://localhost:8000/api/v1/sentiment-analysis/execute" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "I love this product! It works perfectly.",
    "provider": "huggingface",
    "analysis_type": "basic"
  }'
```

**Response:**
```json
{
  "sentiment": "positive",
  "confidence": 0.988,
  "all_scores": {
    "negative": 0.005,
    "neutral": 0.007,
    "positive": 0.988
  },
  "processing_time_seconds": 0.03,
  "provider": "huggingface",
  "model_used": "cardiffnlp/twitter-roberta-base-sentiment-latest"
}
```

#### Comprehensive Analysis (LLM)
```bash
curl -X POST "http://localhost:8000/api/v1/sentiment-analysis/execute" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "The product quality is excellent but shipping was slow and expensive.",
    "provider": "llm",
    "analysis_type": "comprehensive",
    "llm_model": "mistral"
  }'
```

**Response:**
```json
{
  "sentiment": "mixed",
  "confidence": 0.78,
  "emotions": ["satisfaction", "frustration"],
  "aspects": [
    {"aspect": "product quality", "sentiment": "positive", "confidence": 0.9},
    {"aspect": "shipping", "sentiment": "negative", "confidence": 0.8}
  ],
  "key_phrases": [
    {"phrase": "excellent quality", "sentiment": "positive"},
    {"phrase": "slow and expensive", "sentiment": "negative"}
  ],
  "insights": "Customer appreciates product quality but frustrated with shipping experience"
}
```

### Configuration Options

```yaml
# Sentiment analysis flow configuration
inputs:
  - name: "text"              # Text to analyze (required)
  - name: "provider"          # "huggingface" or "llm" (default: huggingface)
  - name: "analysis_type"     # "basic", "detailed", "comprehensive", "emotions"
  - name: "hf_model_name"     # HuggingFace model (optional)
  - name: "llm_model"         # LLM model name (default: mistral)
  - name: "device"            # "auto", "cpu", "cuda" (HuggingFace only)
```

### Performance Comparison

| Provider | Speed | Accuracy | Use Case |
|----------|-------|----------|----------|
| **HuggingFace** | ‚ö° 0.02-0.05s | üìä 95-99% | High-volume, real-time |
| **LLM** | üêå 2-5s | üß† Context-aware | Complex analysis, nuanced text |

### Best Practices

- **Use HuggingFace** for high-volume processing and clear sentiment cases
- **Use LLM** for complex text requiring context understanding
- **Basic analysis** for simple positive/negative classification
- **Comprehensive analysis** for detailed insights and aspect-based sentiment
- **Enable CUDA** for faster HuggingFace processing on GPU systems

## Container Architecture

```yaml
services:
  app:              # FastAPI application
  redis:            # State storage
  celery-worker-*:  # Async processing workers
  celery-flower:    # Worker monitoring
```

## Testing

```bash
# Health check
curl http://localhost:8000/health

# Flow catalog
curl http://localhost:8000/catalog

# Unit tests
pytest tests/ -v
```

## Project Structure

```
promptflow/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ executors/          # AI components
‚îÇ   ‚îú‚îÄ‚îÄ flow_engine/        # Flow orchestration
‚îÇ   ‚îú‚îÄ‚îÄ schema.py          # Data models
‚îÇ   ‚îî‚îÄ‚îÄ state_store.py     # Redis state management
‚îú‚îÄ‚îÄ flows/                 # YAML flow definitions
‚îú‚îÄ‚îÄ tests/                 # Test suite
‚îú‚îÄ‚îÄ main.py               # FastAPI application
‚îú‚îÄ‚îÄ celery_app.py         # Celery configuration
‚îî‚îÄ‚îÄ docker-compose.yml    # Container setup
```

## Prerequisites

- Docker & Docker Compose
- Python 3.11+ (for local development)
- Ollama (for LLM inference)

## License

MIT License - see [LICENSE](LICENSE) file for details.