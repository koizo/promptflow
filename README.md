# AI Inference Platform

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)

Build AI workflows with YAML configuration. Combine OCR, document processing, and LLM analysis through reusable executors that auto-generate REST APIs.

## Case Study: Amazon Q Developer Evaluation

This project serves as a case study to evaluate **Amazon Q Developer** as an autonomous development agent using various MCP servers. While all code was fully generated by Amazon Q, strict rules and guidance were essential to reach this level of maturity.

## Features

- **YAML-Driven Workflows** - Define AI flows using simple YAML configuration
- **Auto-Generated APIs** - REST endpoints created automatically from flow definitions
- **Async Processing** - Celery-based task processing for production workloads
- **Multi-Format Support** - Handle documents, images, and text files
- **Real-Time Status** - Track flow execution progress via Redis state management

## Architecture

```
YAML Flow → Flow Engine → Executors → Auto-Generated API
```

The platform uses **Executors** (reusable AI components) orchestrated by **Flows** (YAML definitions) to automatically generate REST APIs.

## Quick Start

1. **Start the platform**
```bash
git clone <repository>
cd promptflow
docker-compose up -d
```

2. **Test OCR analysis**
```bash
curl -X POST http://localhost:8000/api/v1/ocr-analysis/execute \
  -F "file=@image.jpg"
```

3. **Test document analysis**
```bash
curl -X POST http://localhost:8000/api/v1/document-analysis/execute \
  -F "file=@document.pdf"
```

4. **Test speech transcription**
```bash
curl -X POST http://localhost:8000/api/v1/speech-transcription/execute \
  -F "file=@audio.wav" \
  -F "whisper_provider=local" \
  -F "language=auto"
```

5. **View documentation**
- API Docs: http://localhost:8000/docs
- Flow Catalog: http://localhost:8000/catalog

## Flow Definition

Define workflows in YAML:

```yaml
name: "document_analysis"
description: "Extract and analyze text from documents"

inputs:
  - name: "file"
    type: "file"
    required: true

steps:
  - name: "extract_text"
    executor: "document_extractor"
    config:
      file_path: "{{ inputs.file }}"
      
  - name: "analyze_content"
    executor: "llm_analyzer"
    config:
      text: "{{ steps.extract_text.text }}"

config:
  execution:
    mode: "async"
    timeout: 300
```

## Execution Modes

### Asynchronous (Production)
```yaml
config:
  execution:
    mode: "async"
```

- **Non-blocking** - Returns task ID immediately
- **Celery workers** - Processing in dedicated worker processes
- **Scalable** - Handle multiple concurrent requests
- **Status tracking** - Real-time progress via Redis
- **Recommended for production**

### Synchronous (Development Only)
```yaml
config:
  execution:
    mode: "sync"
```

**⚠️ NOT RECOMMENDED FOR PRODUCTION**

- **Blocking** - Blocks API server during processing
- **No concurrency** - Cannot handle multiple requests
- **Timeout risk** - Long tasks may fail
- **Development only** - Use for testing and debugging

## Available Flows

### OCR Analysis (Async)
- **Endpoint**: `/api/v1/ocr-analysis/execute`
- **Features**: Image text extraction, LLM analysis, status tracking

### Document Analysis (Sync)
- **Endpoint**: `/api/v1/document-analysis/execute`
- **Features**: Multi-format support (PDF, Word, Excel), custom prompts

### Speech Transcription (Async)
- **Endpoint**: `/api/v1/speech-transcription/execute`
- **Features**: Audio-to-text conversion, LLM analysis, multiple Whisper providers
- **Supported Formats**: MP3, WAV, M4A, AAC, OGG, FLAC, WMA, OPUS
- **Providers**: Local Whisper, OpenAI API, HuggingFace Models

## Whisper Providers

The speech transcription flow supports three Whisper providers, each with different advantages:

### 1. Local Whisper (Default)
```bash
curl -X POST http://localhost:8000/api/v1/speech-transcription/execute \
  -F "file=@audio.wav" \
  -F "whisper_provider=local" \
  -F "whisper_model=base"
```
- **Models**: tiny, base, small, medium, large
- **Pros**: No API costs, works offline, good baseline performance
- **Cons**: Limited to OpenAI's original models
- **Best for**: Development, cost-sensitive deployments

### 2. OpenAI Whisper API
```bash
curl -X POST http://localhost:8000/api/v1/speech-transcription/execute \
  -F "file=@audio.wav" \
  -F "whisper_provider=openai"
```
- **Requirements**: OPENAI_API_KEY environment variable
- **Pros**: Highest quality, latest improvements, no local compute
- **Cons**: Per-request costs, requires internet
- **Best for**: Production with quality requirements

### 3. HuggingFace Models (NEW!)
```bash
curl -X POST http://localhost:8000/api/v1/speech-transcription/execute \
  -F "file=@audio.wav" \
  -F "whisper_provider=huggingface" \
  -F "hf_model_name=openai/whisper-large-v3" \
  -F "device=auto"
```

#### Popular HuggingFace Models:
- **openai/whisper-tiny** (39M params, fastest)
- **openai/whisper-base** (74M params, good balance)
- **openai/whisper-large-v3** (1550M params, best quality)
- **distil-whisper/distil-large-v2** (756M params, **6x faster**)
- **Custom fine-tuned models** (username/model-name)

#### Device Options:
- **auto**: GPU if available, fallback to CPU
- **cuda**: Force GPU usage (requires NVIDIA GPU)
- **cpu**: Force CPU usage (slower but universal)

#### Advantages:
- **Latest Models**: Access to newest Whisper variants
- **Faster Options**: Distil-Whisper models with 6x speedup
- **Custom Models**: Use domain-specific fine-tuned models
- **No API Costs**: Run locally without per-request charges
- **Flexibility**: Easy model switching via configuration

## Core Executors

### File Processing
- **FileHandler** - Upload, validation, temp storage
- **DocumentExtractor** - Text extraction from documents
- **ImageHandler** - Image processing

### AI Processing
- **OCRProcessor** - Optical Character Recognition
- **WhisperProcessor** - Speech-to-text transcription (Local, OpenAI, HuggingFace)
- **LLMAnalyzer** - Large Language Model analysis

### Utility
- **ResponseFormatter** - Standardized responses
- **TemplateEngine** - Dynamic configuration

## Extending the Platform

### 1. Create Executor
```python
from .base_executor import BaseExecutor, ExecutionResult

class MyExecutor(BaseExecutor):
    async def execute(self, context):
        result = await self.process_data(context.config.get("input"))
        return ExecutionResult(success=True, data={"output": result})
```

### 2. Register Executor
```python
self.executor_registry.register("my_executor", MyExecutor)
```

### 3. Create Flow
```yaml
name: "my_flow"
steps:
  - name: "process"
    executor: "my_executor"
    config:
      input: "{{ inputs.data }}"
```

### 4. Auto-Generated APIs
- `POST /api/v1/my-flow/execute`
- `GET /api/v1/my-flow/info`
- `GET /api/v1/my-flow/health`

## Container Architecture

```yaml
services:
  app:              # FastAPI application
  redis:            # State storage
  celery-worker-*:  # Async processing workers
  celery-flower:    # Worker monitoring
```

## Testing

```bash
# Health check
curl http://localhost:8000/health

# Flow catalog
curl http://localhost:8000/catalog

# Unit tests
pytest tests/ -v
```

## Project Structure

```
promptflow/
├── core/
│   ├── executors/          # AI components
│   ├── flow_engine/        # Flow orchestration
│   ├── schema.py          # Data models
│   └── state_store.py     # Redis state management
├── flows/                 # YAML flow definitions
├── tests/                 # Test suite
├── main.py               # FastAPI application
├── celery_app.py         # Celery configuration
└── docker-compose.yml    # Container setup
```

## Prerequisites

- Docker & Docker Compose
- Python 3.11+ (for local development)
- Ollama (for LLM inference)

## License

MIT License - see [LICENSE](LICENSE) file for details.